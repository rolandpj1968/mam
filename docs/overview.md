# Multi-Accumulator Machine

The multi-accumulator machine (*MAM*) is an machine- and *instruction set architecture* (ISA) that fits into the general category of explicitly-parallel VLIW machines. MAM goes somewhat further than typical VLIW architectures and ISA's by directly driving multiple mostly-independent execution units from execution-unit-specific (sub-)operations within a single *instruction*. Each (hardware) execution unit is driven by a specific *slot* in the (long) instruction. Each slot within the instruction 'word' is 8 bits; the instruction comprises a collection of these 8-bit operations and the semantics of instruction execution is that all (non-NOP) operands in the instruction word are executed in parallel.

In general a specific MAM *model* comprises a particular collection of execution units of various types - a typical model would include some number of *arithmetic* units, some *memory* (load/store) units and a *flow-control* (branch/call et al) unit.

In order to minimise machine code bloat the execution units and corresponding operation set(s) are exposed as *accumulator* machines - all operations use an implicit 'accumulator' register operand (and possibly one or more other implicit/explicit operands) and all results are written back to the implicit accumulator register. This architectural decision is made deliberately in order to fit a useful set of operations within 8 bits of per-execution-unit operand, allowing many separate execution units to be driven from a relatively reasonable 64-bit (up to 8 parallel execution units) or 128-bit (up to 16 parallel execution units) instruction word size.

In addition, MAM also goes further than typical VLIW architectures in that each execution unit is (almost) entirely self-contained and independent. For example each arithmentic execution unit includes its own register set, and execution unit state - registers, flags - are not directly visible from other execution units. Instead, values are explicitly transfered between execution units as and when required. While this does introduce some extra operations, as does the accumulator approach, the intent is to throw enough parallel execution units at the problem to achieve useful instruction/operation throughput.

As is typical with VLIW architectures, MAM strongly encourages conditional and/or speculative execution in preference to branching. MAM memory units support *conditional* and *speculative* memory loads and stores. Conditional reads and writes are exactly analogous to conditional (register) move instructions (or more generally ubiquitous conditional instruction sets like ARM32). On the other hand speculative memory loads (reads) allow memory load instructions to be hoisted above potentially conflicting (i.e. overwriting) memory store instructions, and speculative memory stores (writes) use a local write buffer to provide a special 'speculative' view of memory by one or some of the memory units, which can be discarded/abandoned subsequently if the speculative path is not actually taken. All of this will be elaborated substantially below.

On the other hand, MAM is intended to be (sensibly) implemented as a (very) shallow pipeline architecture - it is intended that almost all execution unit operations complete in a single clock cycle. Accordingly, all operations that do not complete in a single clock cycle are split into *start-op* and *complete-op* instruction operations. The *start-op* operations initiate an async operation - for example integer divide - and are non-blocking. On the other hand *complete-op* operations will stall until the operation result is actually available - note that this of course stalls the entire instruction stream so all execution units need to wait for the the stalling operation to complete. One special example of an *async* operation in the MAM architecture is memory load in the memory unit(s). Like other cases of multi-clock-cycle- or variable-clock-cycle operations, memory read operations in the memory unit(s) are split into *start-read* and *complete-read* instruction operations, where the semantics is that the *complete-read* operation defines the memory state viewpoint of the read. In this sense *start-read* is speculative and allows memory reads to be hoisted even above potentially conflicting/overwriting memory write operations in order to absorb the natural latency of memory cache hierarchy effects. Again, this will be elaborated further below.


